![](https://upload-images.jianshu.io/upload_images/13575947-76cc29c9fc38faf8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

现在各种神经网络都会用到1x1 conv，它到底起到什么作用呢？要回答这个问题，先从最经典的Inception模型说起，Figure 1是[Inception-ResNet-v2](https://arxiv.org/pdf/1602.07261.pdf)的block架构图。

![Figure 1: https://arxiv.org/pdf/1602.07261.pdf](https://upload-images.jianshu.io/upload_images/13575947-8f508e2b4092ff5b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

### 作用一：改变矩阵维度
block中每个sublayer的第一层都1x1 conv，它在这里的作用之一是降维，减少运算量--feature map channel数越少，卷积操作需要的计算量越小。但就像图像压缩一样，压缩率越大丢帧越多，1x1 conv是如果做到在质量与速度的平衡的呢？

![Figure 2](https://upload-images.jianshu.io/upload_images/13575947-f499bf0da652dd91.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

Figure 2中间图形就是1x1卷积核，左边的矩阵是input，而右边的正方形代表feature map中的一个grid，或一个pixel，它是1x1卷积核其中一个channel：(1x1x1)，与input矩阵对应pixel位置的pixel-wise向量：(1x1x32)的点积的结果。当input矩阵与1x1卷积核完成所有pixel位置（4x4）的点积计算后，会得到一个4x4x1 shape的feature map，grid size不变，整个过程相当于把一个3D矩阵压缩为一个2D矩阵。feature map经过非线性（激活函数）计算后，每个pixel位置就相当于input矩阵对应pixel位置的pixel-wise向量的均值。这就是为什么神经网络会用1x1 conv来改变矩阵维度的原因，无论是降维还是升维，矩阵的性质不变，所以可以构造出各种结构的神经网络

要特别注意的是，这里的一个Conv层并不是只有一个conv()，而是conv()、droupout()、BN()和ReLu()等组合。Conv层是CNN的底层模块，每个神经网络都有自己的定义，例如：
```
def Conv(nin, nf, stride=1):
  return nn.Sequential(
      nn.Conv2d(nin, nf, 3, stride, 1, bias=False),
      nn.BatchNorm2d(nf),
      nn.ReLU(inplace=True)
  )
```

### 作用二：增加非线性

![Figure 3: https://arxiv.org/pdf/1512.03385.pdf](https://upload-images.jianshu.io/upload_images/13575947-303319c6521cb374.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

Figure 3是[Resnet-18/34/50/101/152](https://arxiv.org/pdf/1512.03385.pdf)的block架构图，图中黄色框位置是Resnet-50的一个block，它由三个Conv层组成，开头和结尾都是1x1 conv，三个block组成一个conv2_x层。把这三个block展开你会发现一个很有趣的结构，即block中的第二层、第三层，以及下一个block的第一层组成了一个如Figure 4，称为“bottleneck”的从低纬到高维又到低维的网络，bottleneck是瓶颈的意思，它是从一个空间到另一个大（或小）空间的过度地带，“小->大->小”或“大->小->大”都是bottleneck network。

![Figure 4](https://upload-images.jianshu.io/upload_images/13575947-eb2fa7a204517c82.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

前文已经分析了1x1 conv不会对矩阵的性质产生大影响，Resnet-50为什么要引入bottleneck，而不是像隔壁Resnet-34那样每个block的卷积核维数都是相同的呢？原因有两个：
* 如上前文所述，矩阵降维可以减小所需运算量。深层神经网络的计算量可能会随层数增加呈指数增长，所以，Resnet-50/101/152都引入了1x1conv，1x1卷积核相比3x3卷积核所需计算量更少。
* 增加神经网络非线性程度。增加非线性可以增加神经网络的复杂度，复杂的神经网络才可以更精确地逼近任意函数（或数学模型），而1x1 conv可以用较低运算成本通过改变channel维度为网络增加复杂度。如果对非线性与神经网络的关系有疑问，可以阅读我另一篇博文[你真的明白神经网络是什么?](https://www.jianshu.com/writer#/notebooks/28459409/notes/34480600)。

---

## 总结
1x1卷积层可以在不改变矩阵性质的情况下，可以灵活地变化矩阵channel的维度，不仅可以给矩阵降维减轻运算量，还可以构建bottleneck，低成本地增加网络复杂度使其可以逼近更精确的目标数学模型。

## Refences
* [Inception-ResNet-v2](https://arxiv.org/pdf/1602.07261.pdf)
* [Resnet-18/34/50/101/152](https://arxiv.org/pdf/1512.03385.pdf)
* [你真的明白神经网络是什么?](https://www.jianshu.com/writer#/notebooks/28459409/notes/34480600)










